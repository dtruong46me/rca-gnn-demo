{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f29bdbb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f29bdbb5",
        "outputId": "de0fa528-08d7-421e-cb4b-8149d4a09313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 10 07:49:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Show gpu\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ecd9cf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ecd9cf1",
        "outputId": "1970a239-1814-40c4-cd2f-5f62929a7710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9077bbc0",
      "metadata": {
        "id": "9077bbc0"
      },
      "outputs": [],
      "source": [
        "# config.py\n",
        "import torch\n",
        "\n",
        "class config:\n",
        "    # C·∫•u h√¨nh tham s·ªë Model\n",
        "    HIDDEN_CHANNELS = 64\n",
        "    LEARNING_RATE = 0.001\n",
        "    EPOCHS = 50\n",
        "    DROPOUT = 0.2\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # File paths\n",
        "    NODE_FILE = '/content/dataset_nodes_info.csv'\n",
        "    EDGE_FILE = '/content/dataset_topology_edges.csv'\n",
        "    TICKET_FILE = '/content/dataset_tickets.csv'\n",
        "    MODEL_PATH = '/content/rca_gnn_model.pth'\n",
        "    VECTORIZER_PATH = '/content/vectorizer.pkl'\n",
        "\n",
        "    # C·∫•u h√¨nh Feature\n",
        "    # K√≠ch th∆∞·ªõc vector cho text log (Description)\n",
        "    TEXT_EMBEDDING_DIM = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "18666f11",
      "metadata": {
        "id": "18666f11"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "# import config  <-- Removed import\n",
        "\n",
        "# --- TH√äM CLASS N√ÄY ---\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # inputs: Logits (ch∆∞a qua sigmoid)\n",
        "        # targets: labels (0 ho·∫∑c 1)\n",
        "\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-bce_loss) # pt l√† x√°c su·∫•t d·ª± ƒëo√°n ƒë√∫ng\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "\n",
        "class RootCauseGNN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_channels, num_classes=1):\n",
        "        super(RootCauseGNN, self).__init__()\n",
        "\n",
        "        # Layer 1: GAT Conv\n",
        "        # heads=4 gi√∫p model h·ªçc ƒë∆∞·ª£c nhi·ªÅu m·ªëi quan h·ªá kh√°c nhau\n",
        "        self.conv1 = GATConv(num_node_features, hidden_channels, heads=4, dropout=config.DROPOUT)\n",
        "\n",
        "        # Layer 2: GAT Conv\n",
        "        # Input dim = hidden_channels * heads\n",
        "        self.conv2 = GATConv(hidden_channels * 4, hidden_channels, heads=2, dropout=config.DROPOUT)\n",
        "\n",
        "        # Layer 3: Output Layer\n",
        "        # Tr·∫£ v·ªÅ 1 gi√° tr·ªã duy nh·∫•t (Logit) cho m·ªói node ƒë·ªÉ d√πng BCEWithLogitsLoss\n",
        "        self.conv3 = GATConv(hidden_channels * 2, num_classes, heads=1, concat=False, dropout=config.DROPOUT)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Layer 1\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
        "\n",
        "        # Layer 3\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ Logits (ch∆∞a qua Sigmoid)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fdd813b2",
      "metadata": {
        "id": "fdd813b2"
      },
      "outputs": [],
      "source": [
        "# data_processor.py\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch_geometric.data import Data\n",
        "# import config <-- Removed import\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class TelcoGraphDataset:\n",
        "    def __init__(self, mode='train'):\n",
        "        self.node_mapping = {}\n",
        "        self.reverse_mapping = {}\n",
        "        self.tfidf = TfidfVectorizer(max_features=config.TEXT_EMBEDDING_DIM, stop_words='english')\n",
        "        self.type_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "        # 1. Load Data\n",
        "        self.nodes_df = pd.read_csv(config.NODE_FILE)\n",
        "        self.edges_df = pd.read_csv(config.EDGE_FILE)\n",
        "\n",
        "        # N·∫øu file ticket t·ªìn t·∫°i th√¨ load, kh√¥ng th√¨ ƒë·ªÉ tr·ªëng (cho tr∆∞·ªùng h·ª£p infer sau n√†y)\n",
        "        if os.path.exists(config.TICKET_FILE):\n",
        "            self.tickets_df = pd.read_csv(config.TICKET_FILE)\n",
        "            self.tickets_df['Timestamp'] = pd.to_datetime(self.tickets_df['Timestamp'])\n",
        "        else:\n",
        "            self.tickets_df = pd.DataFrame()\n",
        "\n",
        "        # 2. X·ª≠ l√Ω Vectorizer (FIT TR∆Ø·ªöC khi d√πng)\n",
        "        if mode == 'train':\n",
        "            print(\"Fitting Vectorizers on Training Data...\")\n",
        "            # Fit text features\n",
        "            self.tfidf.fit(self.tickets_df['Description'].fillna(\"\"))\n",
        "            # Fit node static features\n",
        "            self.type_encoder.fit(self.nodes_df[['type', 'vendor']])\n",
        "\n",
        "            # Save vectorizers\n",
        "            with open(config.VECTORIZER_PATH, 'wb') as f:\n",
        "                pickle.dump((self.tfidf, self.type_encoder), f)\n",
        "        else:\n",
        "            # Mode eval/infer: Load vectorizers ƒë√£ train\n",
        "            if os.path.exists(config.VECTORIZER_PATH):\n",
        "                print(\"Loading Vectorizers...\")\n",
        "                with open(config.VECTORIZER_PATH, 'rb') as f:\n",
        "                    self.tfidf, self.type_encoder = pickle.load(f)\n",
        "            else:\n",
        "                raise Exception(f\"Vectorizer file {config.VECTORIZER_PATH} not found! Run training first.\")\n",
        "\n",
        "        # 3. Sau khi ƒë√£ c√≥ encoder, m·ªõi chu·∫©n b·ªã mapping v√† static features\n",
        "        self._prepare_mappings_and_static_data()\n",
        "\n",
        "    def _prepare_mappings_and_static_data(self):\n",
        "        # Map Node ID <-> Integer Index\n",
        "        for idx, row in self.nodes_df.iterrows():\n",
        "            self.node_mapping[row['id']] = idx\n",
        "            self.reverse_mapping[idx] = row['id']\n",
        "\n",
        "        # X√¢y d·ª±ng Edge Index (C·∫•u tr√∫c ƒë·ªì th·ªã tƒ©nh)\n",
        "        src, dst = [], []\n",
        "        for _, row in self.edges_df.iterrows():\n",
        "            if row['Source'] in self.node_mapping and row['Target'] in self.node_mapping:\n",
        "                u, v = self.node_mapping[row['Source']], self.node_mapping[row['Target']]\n",
        "                # ƒê·ªì th·ªã v√¥ h∆∞·ªõng (2 chi·ªÅu) ƒë·ªÉ tin lan truy·ªÅn t·ªët h∆°n\n",
        "                src.extend([u, v])\n",
        "                dst.extend([v, u])\n",
        "        self.edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "\n",
        "        # T·∫°o Static Features (Type, Vendor) cho t·∫•t c·∫£ c√°c node\n",
        "        # L√∫c n√†y self.type_encoder ƒê√É ƒê∆Ø·ª¢C FIT r·ªìi, n√™n g·ªçi transform s·∫Ω kh√¥ng l·ªói\n",
        "        self.static_x = self.type_encoder.transform(self.nodes_df[['type', 'vendor']])\n",
        "\n",
        "    def create_time_windows(self, window_size_min=10):\n",
        "        \"\"\"Chia to√†n b·ªô ticket th√†nh c√°c c·ª≠a s·ªï th·ªùi gian\"\"\"\n",
        "        if self.tickets_df.empty:\n",
        "            return []\n",
        "\n",
        "        start_time = self.tickets_df['Timestamp'].min()\n",
        "        end_time = self.tickets_df['Timestamp'].max()\n",
        "\n",
        "        windows = []\n",
        "        current = start_time\n",
        "        while current < end_time:\n",
        "            next_window = current + pd.Timedelta(minutes=window_size_min)\n",
        "            # L·ªçc ticket trong kho·∫£ng n√†y\n",
        "            mask = (self.tickets_df['Timestamp'] >= current) & (self.tickets_df['Timestamp'] < next_window)\n",
        "            batch_df = self.tickets_df[mask]\n",
        "\n",
        "            # Ch·ªâ l·∫•y window n√†o C√ì ticket (ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian train)\n",
        "            if not batch_df.empty:\n",
        "                windows.append(batch_df)\n",
        "            current = next_window\n",
        "\n",
        "        return windows\n",
        "\n",
        "    def df_to_graph_data(self, batch_df):\n",
        "        \"\"\"Chuy·ªÉn ƒë·ªïi DataFrame ticket c·ªßa 1 c·ª≠a s·ªï th·ªùi gian th√†nh PyG Data\"\"\"\n",
        "        num_nodes = len(self.nodes_df)\n",
        "        labels = np.zeros(num_nodes, dtype=float)\n",
        "\n",
        "        # Dynamic Features (Text Embedding) kh·ªüi t·∫°o b·∫±ng 0\n",
        "        dynamic_x = np.zeros((num_nodes, config.TEXT_EMBEDDING_DIM))\n",
        "\n",
        "        # Map tickets v√†o node t∆∞∆°ng ·ª©ng\n",
        "        for _, row in batch_df.iterrows():\n",
        "            if row['Device_ID'] in self.node_mapping:\n",
        "                idx = self.node_mapping[row['Device_ID']]\n",
        "\n",
        "                # Vector h√≥a Description\n",
        "                # D√πng transform (kh√¥ng fit l·∫°i)\n",
        "                vec = self.tfidf.transform([row['Description']]).toarray()[0] # type: ignore\n",
        "                dynamic_x[idx] += vec\n",
        "\n",
        "                # G√°n nh√£n Root Cause\n",
        "                if row.get('Is_Root_Cause', 0) == 1:\n",
        "                    labels[idx] = 1.0\n",
        "\n",
        "        # K·∫øt h·ª£p Static Features v√† Dynamic Features\n",
        "        # Static (v√≠ d·ª• 10 chi·ªÅu) + Dynamic (16 chi·ªÅu) -> Feature Vector 26 chi·ªÅu\n",
        "        final_x = np.hstack([self.static_x, dynamic_x]) # type: ignore\n",
        "\n",
        "        return Data(\n",
        "            x=torch.tensor(final_x, dtype=torch.float),\n",
        "            edge_index=self.edge_index,\n",
        "            y=torch.tensor(labels, dtype=torch.float)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d3e69fc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3e69fc4",
        "outputId": "018586a3-e644-4297-f65c-54c4f48808c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Initializing Dataset...\n",
            "Fitting Vectorizers on Training Data...\n",
            ">>> Creating Time Windows...\n",
            "Total Windows: 1409. Train: 1127, Test: 282\n",
            ">>> Converting to Graphs...\n",
            "Model Input Features: 31\n",
            "‚öñÔ∏è Adjusted Pos Weight: 120.86\n",
            "\n",
            "--- START TRAINING ---\n",
            "Epoch 005 | Loss: 0.0386 | Val Recall: 0.39 | Val Prec: 0.12 | Val F1: 0.19\n",
            "   >>> New Best Model Saved (F1: 0.19)\n",
            "Epoch 010 | Loss: 0.0356 | Val Recall: 0.36 | Val Prec: 0.12 | Val F1: 0.18\n",
            "Epoch 015 | Loss: 0.0325 | Val Recall: 0.75 | Val Prec: 0.08 | Val F1: 0.15\n",
            "Epoch 020 | Loss: 0.0299 | Val Recall: 0.34 | Val Prec: 0.19 | Val F1: 0.25\n",
            "   >>> New Best Model Saved (F1: 0.25)\n",
            "Epoch 025 | Loss: 0.0286 | Val Recall: 0.75 | Val Prec: 0.19 | Val F1: 0.30\n",
            "   >>> New Best Model Saved (F1: 0.30)\n",
            "Epoch 030 | Loss: 0.0266 | Val Recall: 0.82 | Val Prec: 0.07 | Val F1: 0.12\n",
            "Epoch 035 | Loss: 0.0248 | Val Recall: 0.23 | Val Prec: 0.22 | Val F1: 0.22\n",
            "Epoch 040 | Loss: 0.0233 | Val Recall: 0.55 | Val Prec: 0.25 | Val F1: 0.35\n",
            "   >>> New Best Model Saved (F1: 0.35)\n",
            "Epoch 045 | Loss: 0.0221 | Val Recall: 0.80 | Val Prec: 0.15 | Val F1: 0.25\n",
            "Epoch 050 | Loss: 0.0200 | Val Recall: 0.89 | Val Prec: 0.35 | Val F1: 0.51\n",
            "   >>> New Best Model Saved (F1: 0.51)\n",
            "\n",
            "--- RUNNING DEMO INFERENCE ---\n",
            "Loading Vectorizers...\n",
            "üìÇ Loaded model from /content/rca_gnn_model.pth\n",
            "\n",
            "Testing CASE A (S·ª± c·ªë OSPF) (4 tickets)...\n",
            "üü¢ System Normal (No Root Cause Detected)\n",
            "\n",
            "Testing CASE B (B·∫£o tr√¨) (2 tickets)...\n",
            "üü¢ System Normal (No Root Cause Detected)\n"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.loader import DataLoader\n",
        "# import config <-- Removed import\n",
        "# from data_processor import TelcoGraphDataset <-- Removed import\n",
        "# from model import RootCauseGNN, FocalLoss <-- Removed import\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def train():\n",
        "    print(\">>> Initializing Dataset...\")\n",
        "    dataset_handler = TelcoGraphDataset(mode='train')\n",
        "\n",
        "    print(\">>> Creating Time Windows...\")\n",
        "    # C·ª≠a s·ªï 30 ph√∫t ƒë·ªÉ gom ƒë·ªß ng·ªØ c·∫£nh\n",
        "    all_windows = dataset_handler.create_time_windows(window_size_min=30)\n",
        "\n",
        "    if len(all_windows) == 0:\n",
        "        print(\"ERROR: No data windows created.\")\n",
        "        return\n",
        "\n",
        "    # Split Train/Test\n",
        "    split_idx = int(len(all_windows) * 0.8)\n",
        "    train_windows = all_windows[:split_idx]\n",
        "    test_windows = all_windows[split_idx:]\n",
        "\n",
        "    print(f\"Total Windows: {len(all_windows)}. Train: {len(train_windows)}, Test: {len(test_windows)}\")\n",
        "\n",
        "    print(\">>> Converting to Graphs...\")\n",
        "    train_data_list = [dataset_handler.df_to_graph_data(df) for df in train_windows]\n",
        "    test_data_list = [dataset_handler.df_to_graph_data(df) for df in test_windows]\n",
        "\n",
        "    train_loader = DataLoader(train_data_list, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_data_list, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Setup Model\n",
        "    sample_data = train_data_list[0]\n",
        "    num_features = sample_data.num_features\n",
        "    print(f\"Model Input Features: {num_features}\")\n",
        "\n",
        "    model = RootCauseGNN(num_features, config.HIDDEN_CHANNELS, num_classes=1).to(config.DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "    # --- FIX TR·ªåNG S·ªê (POS_WEIGHT) ---\n",
        "    total_pos = sum([d.y.sum().item() for d in train_data_list])\n",
        "    total_neg = sum([(d.y == 0).sum().item() for d in train_data_list])\n",
        "\n",
        "    # C≈®: pos_weight_val = total_neg / (total_pos + 1e-5) -> Ra 14000 (Qu√° l·ªõn)\n",
        "    # M·ªöI: D√πng cƒÉn b·∫≠c hai ƒë·ªÉ l√†m m∆∞·ª£t (Damping)\n",
        "    pos_weight_val = np.sqrt(total_neg / (total_pos + 1e-5))\n",
        "\n",
        "    # Ho·∫∑c n·∫øu v·∫´n cao, g√°n c·ª©ng m·ªôt con s·ªë h·ª£p l√Ω (v√≠ d·ª• 50.0)\n",
        "    # pos_weight_val = 50.0\n",
        "\n",
        "    pos_weight = torch.tensor([pos_weight_val]).to(config.DEVICE)\n",
        "    print(f\"‚öñÔ∏è Adjusted Pos Weight: {pos_weight.item():.2f}\")\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    print(\"\\n--- START TRAINING ---\")\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(config.DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = criterion(out.squeeze(), batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            val_metrics = evaluate(model, test_loader)\n",
        "            print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | \"\n",
        "                  f\"Val Recall: {val_metrics['recall']:.2f} | \"\n",
        "                  f\"Val Prec: {val_metrics['precision']:.2f} | \"\n",
        "                  f\"Val F1: {val_metrics['f1']:.2f}\")\n",
        "\n",
        "            # L∆∞u model n·∫øu F1 c·∫£i thi·ªán\n",
        "            if val_metrics['f1'] > best_f1:\n",
        "                best_f1 = val_metrics['f1']\n",
        "                torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "                print(f\"   >>> New Best Model Saved (F1: {best_f1:.2f})\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(config.DEVICE)\n",
        "            out = model(batch)\n",
        "            probs = torch.sigmoid(out.squeeze())\n",
        "\n",
        "            # Threshold quan tr·ªçng: TƒÉng l√™n 0.7 ho·∫∑c 0.8 ƒë·ªÉ l·ªçc b·ªõt False Positive\n",
        "            preds = (probs > 0.7).float()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    return {\n",
        "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
        "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
        "        'f1': f1_score(all_labels, all_preds, zero_division=0)\n",
        "    }\n",
        "\n",
        "# --- PH·∫¶N INFERENCE SAU KHI TRAIN ---\n",
        "def run_inference_demo():\n",
        "    print(\"\\n--- RUNNING DEMO INFERENCE ---\")\n",
        "\n",
        "    # 1. Load Data Handler (Mode test ƒë·ªÉ load vectorizer ƒë√£ train)\n",
        "    if not os.path.exists(config.VECTORIZER_PATH):\n",
        "        print(\"‚ö†Ô∏è Ch∆∞a c√≥ file vectorizer, c·∫ßn train tr∆∞·ªõc!\")\n",
        "        return\n",
        "\n",
        "    dataset_handler = TelcoGraphDataset(mode='test')\n",
        "\n",
        "    # 2. Load Model\n",
        "    # C·∫ßn t·∫°o 1 dummy data ƒë·ªÉ l·∫•y num_features kh·ªüi t·∫°o model\n",
        "    dummy_df = pd.DataFrame([{\"Device_ID\": \"DUMMY\", \"Description\": \"\", \"Timestamp\": pd.Timestamp.now()}])\n",
        "    dummy_data = dataset_handler.df_to_graph_data(dummy_df)\n",
        "\n",
        "    model = RootCauseGNN(dummy_data.num_features, config.HIDDEN_CHANNELS, num_classes=1).to(config.DEVICE)\n",
        "\n",
        "    if os.path.exists(config.MODEL_PATH):\n",
        "        model.load_state_dict(torch.load(config.MODEL_PATH, map_location=config.DEVICE))\n",
        "        print(f\"üìÇ Loaded model from {config.MODEL_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Model file not found!\")\n",
        "        return\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # 3. ƒê·ªãnh nghƒ©a Test Cases\n",
        "\n",
        "    # CASE A: S·ª± c·ªë th·∫≠t (Root Cause: AGG-005 b·ªã l·ªói OSPF)\n",
        "    # K√©o theo: ACCESS-0021 (Link Down), ONT-00156 (Lost)\n",
        "    case_incident = [\n",
        "        {\"Device_ID\": \"AGG-005\", \"Description\": \"OSPF State Change to Down\", \"Is_Root_Cause\": \"?\"},\n",
        "        {\"Device_ID\": \"ACCESS-0021\", \"Description\": \"Link Down\", \"Is_Root_Cause\": \"?\"},\n",
        "        {\"Device_ID\": \"ONT-00156\", \"Description\": \"ERROR_Lost 100%\", \"Is_Root_Cause\": \"?\"},\n",
        "        {\"Device_ID\": \"CORE-01\", \"Description\": \"Interface Up\", \"Is_Root_Cause\": \"?\"} # Nhi·ªÖu\n",
        "    ]\n",
        "\n",
        "    # CASE B: B√¨nh th∆∞·ªùng (Ch·ªâ c√≥ b·∫£o tr√¨)\n",
        "    case_normal = [\n",
        "        {\"Device_ID\": \"AGG-010\", \"Description\": \"B·∫£o tr√¨ ƒë·ªãnh k·ª≥ h·ªá th·ªëng\", \"Is_Root_Cause\": \"?\"},\n",
        "        {\"Device_ID\": \"ONT-00888\", \"Description\": \"Kh√°ch h√†ng b√°o ch·∫≠m\", \"Is_Root_Cause\": \"?\"}\n",
        "    ]\n",
        "\n",
        "    # H√†m ph·ª• tr·ª£ ƒë·ªÉ ch·∫°y 1 case\n",
        "    def predict_case(case_name, tickets_list):\n",
        "        print(f\"\\nTesting {case_name} ({len(tickets_list)} tickets)...\")\n",
        "        df_batch = pd.DataFrame(tickets_list)\n",
        "        graph_data = dataset_handler.df_to_graph_data(df_batch)\n",
        "        graph_data = graph_data.to(config.DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(graph_data)\n",
        "            probs = torch.sigmoid(out.squeeze())\n",
        "\n",
        "        # In k·∫øt qu·∫£\n",
        "        found_root = False\n",
        "        for idx, prob in enumerate(probs):\n",
        "            p_val = prob.item()\n",
        "            if p_val > 0.5: # Threshold hi·ªÉn th·ªã\n",
        "                node_id = dataset_handler.reverse_mapping[idx]\n",
        "                # L·∫•y description t·ª´ input n·∫øu c√≥\n",
        "                desc = next((t['Description'] for t in tickets_list if t['Device_ID'] == node_id), \"N/A (No Ticket)\")\n",
        "                print(f\"üî¥ ALERT: {node_id} | Prob: {p_val:.4f} | Log: {desc}\")\n",
        "                found_root = True\n",
        "\n",
        "        if not found_root:\n",
        "            print(\"üü¢ System Normal (No Root Cause Detected)\")\n",
        "\n",
        "    # Ch·∫°y test\n",
        "    predict_case(\"CASE A (S·ª± c·ªë OSPF)\", case_incident)\n",
        "    predict_case(\"CASE B (B·∫£o tr√¨)\", case_normal)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(config.TICKET_FILE):\n",
        "        print(\"Please run generate_data.py first!\")\n",
        "    else:\n",
        "        # 1. Train\n",
        "        train()\n",
        "\n",
        "        # 2. Test Cases\n",
        "        run_inference_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f317498a",
      "metadata": {
        "id": "f317498a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}